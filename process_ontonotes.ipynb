{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone repo if not already done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clone the Ontonotes processing scripts and the word mapping between `male <--> female` characters. Once we've obtained the Ontonotes data we process it using this [link](https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'OntoNotes-5.0-NER-BIO'...\n",
      "remote: Enumerating objects: 27517, done.\u001b[K\n",
      "remote: Total 27517 (delta 0), reused 0 (delta 0), pack-reused 27517\u001b[K\n",
      "Receiving objects: 100% (27517/27517), 101.80 MiB | 3.51 MiB/s, done.\n",
      "Resolving deltas: 100% (19834/19834), done.\n",
      "Checking out files: 100% (26670/26670), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gn_glove'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 199 (delta 2), reused 0 (delta 0), pack-reused 193\u001b[K\n",
      "Receiving objects: 100% (199/199), 67.78 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (88/88), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/uclanlp/gn_glove.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run bash command to process .gold_conll file as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this bash command processes the ontonotes into a csv is much quicker than using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!for path in $(find ./OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/ -name \"*.gold_conll\"); do sed 's/  */,/g; s/\"/\"\"/g' ${path} > ${path}.csv;done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmanela/anaconda3/lib/python3.6/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/danielmanela/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmanela/anaconda3/lib/python3.6/site-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '1.2.1' or newer of 'bottleneck' (version '1.2.0' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load content from csv's as a list of lists, with each sublist\n",
    "    correspoinding to a line in the csv.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        for line in reader:\n",
    "            if len(line) > 0: \n",
    "                if line[0][0] != '#':\n",
    "                    content.append(line)\n",
    "            else:\n",
    "                content.append([])\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_pronoun_map():\n",
    "    \"\"\"\n",
    "    Create pronoun mapping to switch possessive\n",
    "    and personal pronouns to their opposite gender\n",
    "    \"\"\"\n",
    "    pronoun_map_df = pd.DataFrame([\n",
    "        ['he', '[she]', 'PRP'],\n",
    "        ['she', '[he]', 'PRP'],\n",
    "        ['his', '[her]', 'PRP$'],   \n",
    "        ['his', '[hers]', 'PRP'],\n",
    "        ['hers', '[his]', 'PRP'], # Added to counter line 5026 in 'bc/phoenix/00/phoenix_0000.gold_conll.csv'\n",
    "        ['her', '[his]', 'PRP$'],   \n",
    "        ['him', '[her]', 'PRP'],\n",
    "        ['her', '[him]', 'PRP'],\n",
    "    ])\n",
    "    pronoun_map_df.columns = ['word', 'flipped_pronoun', 'pos_0']\n",
    "    return pronoun_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_content(data):\n",
    "    \"\"\"\n",
    "    Select \"word\" and Part of Speech column (\"pos_0\") from data.\n",
    "    Sub in all missing values with a new line, and return this as\n",
    "    a pandas dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.loc[:, [3,4]]\n",
    "    df.columns = ['word', 'pos_0']\n",
    "    df['word'] = df['word'].str.replace('\"\"', '\"')\n",
    "    df['word'] = df['word'].str.strip()\n",
    "    for col in ['word', 'pos_0']:\n",
    "        df.loc[df[col].isnull(), col] = '\\n'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_name_maps():\n",
    "    \"\"\"\n",
    "    Create mapping of male/female names to anonymised entities.\n",
    "    \n",
    "    We add other male/female names to the list if they are not found\n",
    "    in the nltk.names corpus.\n",
    "    \"\"\"\n",
    "    male_names = [name for name in names.words('male.txt')] + ['Saddam', 'Mao']\n",
    "    female_names = [name for name in names.words('female.txt')] + ['Gong']\n",
    "    full_names = set(male_names + female_names)\n",
    "                    \n",
    "    full_name_pairs = [[name, 'E'+str(i)] for i, name in enumerate(full_names)]\n",
    "    \n",
    "    return pd.DataFrame(full_name_pairs, columns=['word', 'entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flipped_gendered_words_map(path):\n",
    "    \"\"\"\n",
    "    Load male/female word files from gn_glove.git file downloaded above\n",
    "    and create a pd.DataFrame which maps words to their equivalents in\n",
    "    the opposite gender.\n",
    "    \n",
    "    We note that there are words which are mapped to multiple others.\n",
    "    We manually select which pairings we want (stored in `manual_additions`)\n",
    "    and add this to the mapping to the deduplicated original dataframe.\n",
    "    \"\"\"\n",
    "    male_words = []\n",
    "    female_words = []\n",
    "    with open(os.path.join(path, 'male_word_file.txt')) as f:\n",
    "        for line in f:\n",
    "            male_words.append(line.strip('\\n'))    \n",
    "    with open(os.path.join(path, 'female_word_file.txt')) as f:\n",
    "        for line in f:\n",
    "            female_words.append(line.strip('\\n'))\n",
    "              \n",
    "    # Manually add words not in Zhao's subset\n",
    "    male_words = male_words + ['kingdom']\n",
    "    female_words = female_words + ['queendom']      \n",
    "              \n",
    "    full_mapping = [[m, w] for m, w in zip(male_words, female_words)] + \\\n",
    "        [[w, m] for m, w in zip(male_words, female_words)]\n",
    "        \n",
    "              \n",
    "    full_mapping_df = pd.DataFrame(full_mapping, columns=['word', 'flipped_gender_word'])\n",
    "    \n",
    "    # Remove gendered pronoun words\n",
    "    full_mapping_df = full_mapping_df.loc[~full_mapping_df['word'].str.contains('^he$|^she$|^her$|^his$|^him$')]\n",
    "    \n",
    "    # Remove all duplicated 'word' entries and manually re-add those which make most sense\n",
    "    removed_words = set(\n",
    "        full_mapping_df.loc[full_mapping_df['word'].duplicated(keep=False), 'word']\n",
    "    )\n",
    "    full_mapping_df = full_mapping_df.drop_duplicates(subset='word', keep=False)\n",
    "       \n",
    "    manual_additions = pd.DataFrame([\n",
    "        ['bachelor', 'maiden'],\n",
    "        ['bride' , 'bridegroom'],\n",
    "        ['brides' , 'bridegrooms'],\n",
    "        ['dude', 'chick'],\n",
    "        ['dudes', 'chicks'],    \n",
    "        ['gal', 'guy'],\n",
    "        ['gals', 'guys'],\n",
    "        ['god', 'goddess'],\n",
    "        ['grooms', 'brides'],\n",
    "        ['hostess', 'host'],\n",
    "        ['ladies', 'gentlemen'],\n",
    "        ['lady', 'gentleman'],\n",
    "        ['lass', 'lad'],\n",
    "        ['manservant', 'maid'],\n",
    "        ['mare', 'stallion'],\n",
    "        ['maternity', 'paternity'],\n",
    "        ['paternity', 'maternity'],\n",
    "        ['penis', 'vagina'],\n",
    "        ['mistress', 'master'],\n",
    "        ['nun', 'priest'],\n",
    "        ['nuns', 'priests'],   \n",
    "        ['priest', 'priestess'],  \n",
    "        ['priests', 'priestesses'],  \n",
    "        ['prostatic_utricle', 'womb'],\n",
    "        ['sir', 'madam'],\n",
    "        ['wife', 'husband']\n",
    "    ], columns=['word', 'flipped_gender_word'])\n",
    "    \n",
    "    # Ensure all duplicated words are accounted for\n",
    "    assert set(manual_additions['word']) == removed_words\n",
    "    \n",
    "    full_mapping_df = pd.concat([full_mapping_df, manual_additions], axis=0)\n",
    "    \n",
    "    return full_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_full_string_cols(d):\n",
    "    \"\"\"\n",
    "    Unify all anonymised entities, gender flipped words and ungendered\n",
    "    words into a single column.\n",
    "    \"\"\"\n",
    "    d['original_str'] = d['word']\n",
    "    d.loc[d['entity'].notnull(), 'original_str'] = d['entity']\n",
    "    d.loc[d['orig_pronoun'].notnull(), 'original_str'] = d['orig_pronoun']\n",
    "    \n",
    "    d['flipped_str'] = d['word']\n",
    "    d.loc[d['flipped_entity'].notnull(), 'flipped_str'] = d['flipped_entity']\n",
    "    d.loc[d['flipped_pronoun'].notnull(), 'flipped_str'] = d['flipped_pronoun']\n",
    "    d.loc[d['flipped_gender_word'].notnull(), 'flipped_str'] = d['flipped_gender_word']\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_ontonotes_file(path):\n",
    "    \"\"\"\n",
    "    Process ontonotes file located by `path` and process the file as a dataframe.\n",
    "    Flip gendered words and anonymise entities. Concatenate all words in \n",
    "    `orignal_str` and `flipped_st` to create full original and flipped \n",
    "    strings which are returned as an output.\n",
    "    \"\"\"\n",
    "    data = load_data(path)\n",
    "    df = preprocess_content(data)\n",
    "    \n",
    "    pronoun_map = generate_pronoun_map()\n",
    "    name_map = generate_name_maps()\n",
    "    flipped_map = flipped_gendered_words_map('gn_glove/wordlist/')\n",
    "    \n",
    "    df_2 = pd.merge(df, name_map, on='word', how='left')\n",
    "\n",
    "    df_2['word'] = df_2['word'].str.lower()\n",
    "    df_2['flipped_entity'] = 'FL_' + df_2['entity'].str[1:]\n",
    "        \n",
    "    df_2['orig_pronoun'] = '[' + df_2.loc[\n",
    "        (df_2.loc[:, 'pos_0'].astype(str).str.contains('PRP')) &\n",
    "        (df_2.loc[:, 'word'].astype(str).str.contains('^he$|^she$|^her$|^his$|^him$')),\n",
    "        'word'\n",
    "    ].astype(str) + ']'\n",
    "    \n",
    "    df_3 = pd.merge(df_2, pronoun_map, on=['word', 'pos_0'], how='left')\n",
    "\n",
    "    df_4 = pd.merge(df_3, flipped_map, on='word', how='left')\n",
    "    \n",
    "    df_5 = unify_full_string_cols(df_4)\n",
    "\n",
    "    original_string = df_5['original_str'].str.cat(sep=' ')\n",
    "    flipped_string = df_5['flipped_str'].str.cat(sep=' ')    \n",
    "\n",
    "    return original_string, flipped_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through all files and process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13331 [00:00<?, ?it/s]/Users/danielmanela/anaconda3/lib/python3.6/site-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '2.6.2' or newer of 'numexpr' (version '2.6.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "100%|██████████| 13331/13331 [15:36<00:00, 14.70it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/\"\n",
    "EXT = \"*.csv\"\n",
    "all_csv_files = [file\n",
    "                 for path, subdir, files in os.walk(PATH)\n",
    "                 for file in glob(os.path.join(path, EXT))]\n",
    "\n",
    "original_strings = []\n",
    "flipped_strings = []\n",
    "erroneous_paths = []\n",
    "for path in tqdm(all_csv_files):\n",
    "    try:\n",
    "        original, flipped = process_ontonotes_file(path)\n",
    "        original_strings.append(original)\n",
    "        flipped_strings.append(flipped)\n",
    "    except:\n",
    "        erroneous_paths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine sentences until a gendered pronoun appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def compile_pronoun_strings(corpus):\n",
    "    \"\"\"\n",
    "    Note that BERT does not process strings longer than 512 characters. Thus\n",
    "    we ensure that all strings are below this character limit.\n",
    "    \n",
    "    We attempt to add as many sentences as possible to the training example\n",
    "    to provide maximal context to the BERT masked language model. We also\n",
    "    add `[CLS]` and `[SEP]` tokens to our training strings.\n",
    "    \"\"\"\n",
    "    stored_full_strings = []\n",
    "    temp_storage = []\n",
    "    pronouns = ['[his]', '[her]', '[him]', '[she]', '[he]']\n",
    "    pronoun_regex = '\\[his\\]|\\[her\\]|\\[him\\]|\\[she\\]|\\[he\\]|\\[hers\\]'\n",
    "    \n",
    "    num_corpi_too_long = 0\n",
    "    for subset in corpus:\n",
    "        temp = subset.split('\\n')\n",
    "        for string in temp:\n",
    "            temp_storage.append(string)\n",
    "            if re.search(pronoun_regex, string):\n",
    "                if len(' [SEP] '.join(temp_storage)) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-8:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-8:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-7:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-7:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-6:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-6:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-5:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-5:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-4:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-4:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-3:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-3:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-2:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-2:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                elif len(' [SEP] '.join(temp_storage[-1:])) <= 512:\n",
    "                    stored_full_strings.append('[CLS] '+' [SEP] '.join(temp_storage[-1:]) + ' [SEP]')\n",
    "                    temp_storage = []\n",
    "                else:\n",
    "                    num_corpi_too_long += 1\n",
    "                    stored_full_strings.append('___TEXT-TO-LONG___')\n",
    "                    temp_storage = []\n",
    "    print(\"Number of text corpuses which are too long for BERT: {} / {}\".format(num_corpi_too_long, len(corpus)))\n",
    "    return stored_full_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text corpuses which are too long for BERT: 38 / 13331\n",
      "Number of text corpuses which are too long for BERT: 38 / 13331\n",
      "CPU times: user 937 ms, sys: 17.8 ms, total: 955 ms\n",
      "Wall time: 966 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "original_pronoun_strings = compile_pronoun_strings(original_strings)\n",
    "flipped_pronoun_strings = compile_pronoun_strings(flipped_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking processing is symmetric between original and flipped strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24322\n",
      "24322\n",
      "166519\n",
      "166519\n"
     ]
    }
   ],
   "source": [
    "print(len(original_pronoun_strings))\n",
    "print(len(flipped_pronoun_strings))\n",
    "assert len(original_pronoun_strings) == len(flipped_pronoun_strings)\n",
    "\n",
    "orig = [s for strings in original_strings for s in strings.split('\\n')]\n",
    "flip = [s for strings in flipped_strings for s in strings.split('\\n')]\n",
    "\n",
    "print(len(orig))\n",
    "print(len(flip))\n",
    "assert len(orig) == len(flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_training_data(strings):\n",
    "    \"\"\"\n",
    "    Identify whether a string contains a gendered pronoun.\n",
    "    If so, identify if a pronoun is missing, and replace its\n",
    "    occurance with `[MASK]`, whilst keeping the pronoun as the\n",
    "    predictive target.\n",
    "    \n",
    "    If a string has multiple pronouns, we create as many training\n",
    "    examples as there are unique pronouns in the sentence.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    regex = '\\[his\\]|\\[her\\]|\\[him\\]|\\[she\\]|\\[he\\]|\\[hers\\]'\n",
    "\n",
    "\n",
    "    for string in strings:\n",
    "        string_pronouns = re.findall(regex, string)\n",
    "        if string_pronouns:\n",
    "            for pronoun in string_pronouns:\n",
    "                regex_pronoun = re.compile('\\[' + pronoun + '\\]')\n",
    "                temp_str = re.sub(regex, '[MASK]', string)\n",
    "                temp_str = re.sub(r'\\s+', r' ', temp_str)\n",
    "                temp_data = [temp_str, pronoun[1:-1]]\n",
    "                data_list.append(temp_data)\n",
    "        else:\n",
    "            pass   # Pass if no string is present\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_data = generate_training_data(original_pronoun_strings)\n",
    "flipped_data = generate_training_data(flipped_pronoun_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(original_data) == len(flipped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] because this area is a patents minefield it 's impossible to create a product which from start to finish does n't infringe on anyone else 's patents says johnsee E2431 . [SEP] in [MASK] view this problem may be resolved in the future by the exchange of patented technologies between players . [SEP]\",\n",
       " 'his']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some rows are duplicated due to multiple masks in the same line. We deal with these here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_df = pd.DataFrame(original_data, columns=['text', 'pronouns'])\n",
    "flipped_df = pd.DataFrame(flipped_data, columns=['text', 'pronouns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33545, 2)\n",
      "(33545, 2)\n"
     ]
    }
   ],
   "source": [
    "print(original_df.shape)\n",
    "print(flipped_df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We deduplicate these examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27257, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dropped_df = original_df.drop_duplicates(keep='first')\n",
    "original_dropped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26796, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_dropped_df = flipped_df.drop_duplicates(keep='first')\n",
    "flipped_dropped_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are fewer entries in the `flipped_dropped_df` (as `\"her\"` is both a possessive and personal pronoun, whereas men have `\"his\", \"him\"`), we will subselect the data which is only included in `flipped_dropped_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intersection_ind = flipped_dropped_df.index.intersection(original_dropped_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_dropped_df = original_dropped_df.loc[intersection_ind]\n",
    "flipped_dropped_df = flipped_dropped_df.loc[intersection_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the number of training examples in both datasets are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert original_dropped_df.shape == flipped_dropped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26647, 2)\n"
     ]
    }
   ],
   "source": [
    "print(flipped_dropped_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_dropped_df.to_csv('original_data.csv', index=False)\n",
    "flipped_dropped_df.to_csv('flipped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data saved appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('original_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('flipped_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
